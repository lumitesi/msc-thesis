%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter4.tex
%% NOVA thesis document file
%%
%% Chapter with lots of dummy text
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation of the \textit{iCBD-Replication and Cache Server}}
\label{cha:replication}

This chapter addresses the implementation of the central topics of this thesis, divided into two major fields.
The first section talks about the creation of a middleware system that provides replication features in an integrated way to the iCBD platform. A detailed description of the concept and architectural model, as well as the implementation decisions, can be found in this section.
Then, we show how the performance of the platform clients can be heightened by setting up a client-side caching system that stores images adjacent to the consumers. Concluding in exploring the challenges of recreating the complete platform in a new environment and implementing a real-world scenario at Nova University Computer Science department laboratories.


%%-------------------------------------------------------------------
%%	4 - Implementation of a Replication Module
%%-------------------------------------------------------------------
\section{Implementation of a Replication Module}
\label{sec:replication_impl}

One of the central objectives of iCBD is to provide a platform that can be both cloud-centric, with the administration and a portion of the storage burden gathered in a public cloud, or fully hosted on client premises. Either way, it becomes evident that data locality is an important subject, which means that there is the need to study how this data will flow between the multiple components of the iCBD platform.
As can be imagined this is a data-intensive platform, bosting multiple storage devices in many networks and an array of consumers demanding that data at any given time.
All these factors allied to the platform architecture result in the need to create a new component, whose chief mission is to ensure that the data is correctly replicated in the appropriate places, maintaining the consistency of the various versions of VM images stored.

%%-------------------------------------------------------------------
%%	4. - Requirements of the Module
%%-------------------------------------------------------------------
\subsection{Requirements of the Module}
\label{sub:requirements_icbdrep}

Since the beginning of this work the file system to be used as storage was set. Not because is belived to be the best for this type of work, but because is belived that is one of the best. And since the grandure of the project there is an analogous thesis doing work with other type of file systems, a distributed object storage oriented one named Ceph~\cite{Weil2006}.


The are some good reasons for using the BTRFS File System some that only will show up in the decour of this document, but we can enunciate two that are fundamental. The foremost is the support for snapshots

Requirements

The file system is set BTRFS will be used.
Many reasons for that:
The most important is the support for snapshots
Compression


%%-------------------------------------------------------------------
%%	4. - Preliminary tests on the BTRFS Incremental Backup features
%%-------------------------------------------------------------------
%\subsubsection{Preliminary tests on the BTRFS Incremental Backup features}
\paragraph{Preliminary tests on the BTRFS Incremental Backup features}
\label{subsub:pre_test_btrfs}

%https://btrfs.wiki.kernel.org/index.php/Incremental_Backup
%https://www.samba.org/ftp/rsync/rsync.html

The first step is to try to understand the most efficient way to transfer this peculiar kind of data. Given the fact that we are working with a file system with snapshots capabilities, we want to take advantage of this functionality and minimise the amount of data roaming the network. In this sense, we next present some preliminary tests in multiple ways of transferring snapshots between BTRFS file systems both in the same machine and in different ones.
The results obtained here conjugated with the defined requirements are essential for defining the architecture of the replication module steering the implementation at its best path.

Before the start of any implementation, there was the need to validate the capabilities of the BTRFS file system regarding sending snapshots across different systems.
As one of the requirements was the efficiency of the transference of data. So a small comparison was in order. 


\subsection{System Overview}
\label{sub:system_overview}

%%-------------------------------------------------------------------
%%	4. - Image Repository
%%-------------------------------------------------------------------
\subsection{Image Repository}
\label{sub:rep_image_repo}

\subsubsection{iCBD Snapshot Structure}
\label{subsub:icbd_snapshot}

In the replication module we treat a snapshot not only as raw data, but a collection of data and metadata that is essencial to unequivocally distinguish the multiple images present in the system. 


%%-------------------------------------------------------------------
%%	4. - Communications between image repositories
%%-------------------------------------------------------------------
\subsection{Communications between image repositories}
\label{sub:rep_rpcs}


%%-------------------------------------------------------------------
%%	4. - Pyro4 Library
%%-------------------------------------------------------------------
\subsubsection{Pyro4 Library}
\label{subsub:rep_pyro4}

%https://pythonhosted.org/Pyro4/


%%-------------------------------------------------------------------
%%	4. - Master Node
%%-------------------------------------------------------------------
\subsection{Master Node}
\label{sub:rep_master_node}

\subsubsection{CLI Interface}
\label{subsub:rep_cli_interface}

\subsubsection{REST API}
\label{subsub:rep_restapi}

%%-------------------------------------------------------------------
%%	4. - Replica Node
%%-------------------------------------------------------------------
\subsection{Replica Node}
\label{sub:rep_replica_node}






%%-------------------------------------------------------------------
%%	4. - Building a iCBD Cache Server
%%-------------------------------------------------------------------
\section{Building a iCBD Cache Server}
\label{sec:cache_server}

%To solve some of the enunciated problems with a DaaS solution that derive from limited bandwidth, latency and jitter from the limitation of accessing the image repositories from an internet connection and provide some scalability feature with the implementation of proximity cache servers are key. These cache servers can store replicas of the iMIs created and maintained in an administration server. Moreover, since they are located in the same LAN segment as the clients is from here that they will boot.
%For accomplishing this work, the cache servers need to have hard drives. (Although it would be possible to have diskless cache servers, they would be blocked if there was an interruption in the internet access, and it is to avoid that the local drives are necessary. )

%A instalação, configuração e administração dos cache-servers far-se-á também pela instanciação a partir da imagem de uma VM especialmente preparada para o efeito e residente na cloud, no sistema de administração. Os cache-servers arrancam inicialmente pela rede a partir do servidor na cloud e carregam um Linux que vai formatar os discos, neles instalando o conteúdo da própria imagem de VM, terminando com a instalação de um boot loader que irá arrancar o sistema do cache-server após a máquina fazer reboot. Daí em diante o sistema de administração na cloud irá disponibilizar as actualizações que forem necessárias, podendo mesmo forçar a re-instalação total dos cache-servers.

%Note-se que uma vez instalado um cache-server numa LAN, dada a grande fiabilidade que pode ser ainda reforçada através das técnicas habituais em sistemas tolerantes a faltas e de elevada disponibilidade (referir a sugerida pelo Paulo), torna-se viável a utilização de servidores diskless, instanciados a partir de VMs templates configuradas e administradas na cloud e depois replicadas para o cache-server, como nas imagens dos desktops.

%Assim como o Linux dos cache-servers é instalado nos discos locais dos servidores a partir de imagens na cloud, é possível fazer o mesmo com qualquer outra imagem de Linux que se queira. Assim, o administrador de sistema de um cliente poderá configurar outras VMs com o software e os serviços de que necessitar, designar uma máquina física como alvo, e fazer com que a VM seja vertida para os discos da sua máquina física, sendo configurado um boot loader para lhe permitir arrancar com essa configuração.

%Como os clientes nestas categorias tipicamente necessitam de dezenas, centenas ou mais de postos de trabalho, na prática é impossível suportá-los directamente a partir de repositórios remotos de VMs templates. Em vez disso, iremos recorrer a “servidores de proximidade” ou “c​ache-servers”​, máquinas Linux reais ou virtuais devidamente dimensionadas e configuradas, instaladas na LAN dos clientes, idealmente uma (ou mais) por cada segmento de rede, as quais manterão réplicas das VMs templates a que cada cliente tem acesso nos repositórios remotos.
%Com estes cache-servers localmente acessíveis aumentar-se-á drasticamente a velocidade de transferência de dados e a latência e o jitter nos acessos reduzir-se-ão a valores insignificantes, podendo cada cache-server suportar dezenas (ou mais) de PCs clientes (estimamos o número em 15 a 30 por cada interface gigabit nos cache-servers, estimativa que permanece válido para PCs clientes ligados a 100 Mbps através de switches com ligação gigabit aos cache-servers).


\subsection{The infrastructure}
\label{sub:infrastructure}

%Machines List:
%TODO
%Servers - 2x HP ProLiant DL380 Gen9
%Switch - HPE flexfabric 5700 jg898a
%Disk array - HPE MSA 2040 SAN Storage

\begin{table}[htpb]
\centering
\begin{tabular}{lcc}
\hline
                               & \textbf{FCT NOVA}          & \textbf{Reditus}           \\ \hline
\textit{\textbf{Servers}}      & 2 x HP ProLiant DL380 Gen9 & 2 x HP ProLiant DL380 Gen9 \\
\textit{\textbf{Switch}}       & HPE flexfabric 5700 jg898a & HPE flexfabric 5700 jg898a \\
\textit{\textbf{Disk Array}}   & HPE MSA 2040 SAN Storage   & -                          \\
\textit{\textbf{WorkStations}} & 2 Labs (15 PCs each)       & 3 Test PCs                 \\
\textit{\textbf{Networking}}   & 1 Gbps                     & 1 Gbps                     \\ \hline
\end{tabular}
\caption{Physical infrastructure in both sites}
\end{table}


\subsection{System Overview}
\label{sub:system_overview}

%%-------------------------------------------------------------------
%%	4. - Services
%%-------------------------------------------------------------------
\subsection{Services}
\label{sub:cache_services}


%%-------------------------------------------------------------------
%%	4. - Networking
%%-------------------------------------------------------------------
\subsection{Networking}
\label{sub:cache_networking}

%%-------------------------------------------------------------------
%%	4. - Extra Efforts
%%-------------------------------------------------------------------
\subsection{Extra Efforts}
\label{sub:extra_efforts}

Found a Centos 7 kernel bug.
%https://bugs.centos.org/view.php?id=14228
%https://bugzilla.redhat.com/show_bug.cgi


GitLab

Since the work mainly goes around replication and infrastructure problems, makes all sense to think in how the base code is handled. Thinking on this subject and evaluating the code backup system in place (talk about what is the system in place), an svc system is ideal to what we what to accomplish.
So a GitLab on-premises system was deployed and configured. Also configured multiple repositories that will back each module of the iCBD platform.
There are two main objectives of this premise.
First, provide a safe environment for backing up all the base code of the modules. As well as provide versioning control of the that same code.
Second, facilitate a way to replicate the base code of the multiple modules though the various infrastructures running the icbd platform in a clean and transparent way.
Talk a bit of git vantages. (Replication possibilities, backing with the cloud..)
How is implemented? (VM in reditus infra..)



