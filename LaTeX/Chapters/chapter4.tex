%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter4.tex
%% NOVA thesis document file
%%
%% Chapter with lots of dummy text
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation of the \textit{iCBD-Replication and Cache Server}}
\label{cha:impl_replication_caching}

This chapter addresses the implementation of the central topics of this thesis, divided into two major fields.
We first start with a section talking about the creation of a middleware system that provides replication features in an integrated way to the iCBD platform. While providing a detailed description of the concept and architectural model, as well as the implementation decisions, made along the accomplishment of this contribution.
Next, we show the work done on improving the performance of the platform clients. Displaying how setting up a client-side caching system that stores images adjacent to the consumers obtain that sought enhancements. Concluding in exploring the challenges of recreating the complete platform in a new environment and implementing a real-world scenario at Nova University Computer Science department laboratories.

%will at last state the problem and motivation for introduction replication techniques in the storage components of the platform. Moreover, the section prefaces the importance of the implementation of cache servers that hold part of the distribution burden and crucial for the support of an increased number of clients.

This chapter is partitioned as follows :

\begin{description}
    %
    \item [Section~\ref{sec:impl_replication}] overviews the implementation of the middleware dubbed iCBD-Replication. Beginning the journey through the initial architectural process and then showing the implemented components and their interaction with the several layers of the platform.
    %
    \item [Section~\ref{sec:impl_cache_server}] shows how the complete iCBD platform was installed in the NOVA University cluster. Then, a description of the work performed to include a client-side caching server directly connected to the final clients.
    %
\end{description}
\newpage



%%-------------------------------------------------------------------
%%	4 - Implementation of a Replication Module
%%-------------------------------------------------------------------
\section{Implementation of a Replication Module}
\label{sec:impl_replication}

One of the central objectives of iCBD is to provide a platform that can be both cloud-centric, with the administration and a portion of the storage burden gathered in a public cloud, or fully hosted on client premises. Either way, it becomes evident that data locality is an important subject, which means that there is the necessity to study how this data will flow between the multiple components of the iCBD platform.
As can be imagined, this is a data-intensive platform, boasting multiple storage devices in many networks and an array of consumers demanding that data at any given time.

All these factors allied to the platform architecture result in the need to create a new component, whose chief mission is to ensure that the data is correctly replicated in the appropriate places, maintaining the consistency of the various versions of the iCBD Machine Images stored.



%%-------------------------------------------------------------------
%%	4. - Requirements of the Module
%%-------------------------------------------------------------------
\subsection{Requirements of the Module}
\label{sub:requirements_icbdrep}

Since the beginning of this work, the file system selected for use in the storage layer was selected, this is due to the fact that, there was already a functioning prototype of the core iCBD platform making use of BTRFS for all data storage matters. The most critical feature for the operation of the platform is that the file system supports snapshots. BTRFS is a modern file system based on the copy-on-write (COW) principle capable of creating lightweight copies of a file. We detailed the importance of this trait in Section~\ref{sub:icbd_architecture_storage}.
% TODO Cap4 Requirements Citation needed on COW
 
The condition described above applies only in the choice of the File System, in theory, any File System that supports snapshots can be employed in the platform. That is, in fact, the case with the work developed in a dissertation carried out in parallel to this one, where the focus is the use of an object-oriented file system, in particular, the CEPH File System.
Due to this imposition, it is key that this work makes the best use of the BTRFS features, exploring the incremental backup capabilities. More, the replication process should fully integrate with the core platform that already distributes iMIs to clients. Preservation of consistency of the iMIs is also a concern, assuring the distribution of new versions when they are created.

Moreover, it should be taken in account the locality of the data, since the communications could originate and end in the same data centre and the same local network, or happening between different locations that can be in opposite sides of the world. Such aspects as the bandwidth used and the encryption of the data becomes essential to address, requiring the examination of several compression algorithms that can be accommodated to the way the data is processed and also ways of keeping this data secure by encrypting the communications.
\newpage

%Requirements
%The file system is set BTRFS will be used.
%Many reasons for that:
%The most important is the support for snapshots
%Compression


%\subsubsection{Preliminary tests on the BTRFS Incremental Backup features}
\paragraph{Experimentation with the BTRFS Incremental Backup features}
\label{par:incremental_btrfs}

A first step is trying to understand the most efficient way to transfer this unique kind of data (i.e. an iMI). Given the fact that we are working with a file system with snapshots capabilities, we want to take advantage of this functionality and minimise the amount of data roaming the network.

The BTRFS developers provide a userspace set of utilities that can manage BTRFS filesystems, called \texttt{btrfs-progs}. Within that set of tools, there is a pair of commands, \texttt{btrfs send}~\cite{btrfs_send}, and \texttt{btrfs receive}~\cite{btrfs_receive}, that provides the capability to transport data via a stream and employ those differences in a remote filesystem. 

The send command facilitates the process of generating a stream of instructions that describe changes between two subvolume snapshots. Also available in the command is the ability to use an incremental mode, where given a parent snapshot that is available in both send and receive sides, only the small delta between snapshots (e.x. \textit{V2} and \textit{V2-1} in fig~\ref{fig:imi_snap}) is going to integrate the stream. This feature is outstanding since considerably reduces the amount of information that needs to be transferred to reconstruct the snapshot in the receiving end. The send side operations occur in-kernel, beginning by determining differences within subvolumes and based on those differences the kernel generates a set of instructions in a custom formulated stream.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=4in]{cap4_iMI_deltas}
    \caption{iCBD iMI Snapshots Structure}
    \label{fig:imi_snap}
\end{figure}
\newpage

On the remote end, the received command accepts the stream generated by the send command and uses that data to recreate one or more snapshots. Contrary to the send command, receive executes in user space, replaying the instructions that come in the stream one by one, these instructions include the most relevant calls found in a Virtual File System, with Unix system calls like \texttt{create()}, \texttt{mkdir()}, \texttt{link()}, \texttt{symlink()}, \texttt{rename()}, \texttt{unlink()}, \texttt{write()}, along with others.~\cite{btrfs_design}


%https://btrfs.wiki.kernel.org/index.php/Incremental_Backup
%https://www.samba.org/ftp/rsync/rsync.html

%The first step is to try to understand the most efficient way to transfer this peculiar kind of data. Given the fact that we are working with a file system with snapshots capabilities, we want to take advantage of this functionality and minimise the amount of data roaming the network. In this sense, we next present some preliminary tests in multiple ways of transferring snapshots between BTRFS file systems both in the same machine and in different ones.
%The results obtained here conjugated with the defined requirements are essential for defining the architecture of the replication module steering the implementation at its best path.

%Before the start of any implementation, there was the need to validate the capabilities of the BTRFS file system regarding sending snapshots across different systems.
%As one of the requirements was the efficiency of the transference of data. So a small comparison was in order. 



%%-------------------------------------------------------------------
%%	4. - System Overview
%%-------------------------------------------------------------------
\subsection{System Overview}
\label{sub:system_overview}


\begin{figure}[htbp]
	\centering
	\includegraphics[height=4in, width=\textwidth]{cap4_icbd_arquit_draft}
	\caption{iCBD Replication Architecture (Draft)}
	\label{fig:icbd_rep_architecture}
\end{figure}

Since the replication module needs to interact with several bash scripts, with the core iCBD platform, including command line tools and some operating systems low-level features, we betted that the most suitable approach was creating a python distributed middleware using a master-replica paradigm.

Python as a programming language enjoys some valid idiosyncrasies, functioning as an object-oriented language, possessing an extensive standard library and enjoys a big community delivering packages with a wide range of functionality, all facts that contribute to make it the best programming language to bundle everything together.

The several modules of the middleware comprise the main functionalities allowing a node to behave as a Master or Replica node, where each one maintains its individual Image Repository. Then there are also a number of libraries that were developed to interface with some tools, as the BTRFS tool talked about above~\ref{par:incremental_btrfs}, to interface with an SSH connection, build wrappers for compression algorithms and even provide a REST API.

An overview with a small description of the functionality of each component of the system follows:

\begin{description}
	\item [Master Node] This node acts both as a controller to the replication system and an interface to interact with a client whether through a CLI or REST API. Is required that this node reside on an iCBD Administration machine since this is the node that will send changes made to an iMI to all replicas. It is also responsible for providing a communication interface with the Name Server, in this sense when an administrator wants information about which nodes are present on the platform, or what is their status; these requests obtain a route through the master node.
	%
	\item [Replica Node] The main task of this node is to maintain the subscribed list of iMIs updated, receiving the last version of those machine images and store them in the file system as soon as the Master Node makes the available. At request, the Replica Node can deliver the list of iMIs that is storing (including and very important the version numbers of the iMIs present in the node). As well as at any time subscribe to a new iMI which from that point forward will include the roster.
	%
	\item [Name Server] The name server service will run parallel to the Master Node, holding a phone book style record listing all the nodes in the replication system. Nodes register themselves in the name server during the startup process, and at any moment can query the location of another node. All this information is deposited in a simple database.
	%
	\item [Keep Alive] The keep-alive module consists of a thread, launched by the master node, tasked with periodically check if the replica nodes are still working correctly. When its identified that a replica node stopped responding, this service immediately sends a request to the Name Server for the removal of the dead node from the directory.
	%
	\item [Image Repository] This module is a custom-made data structure to hold a large set of iCBD Snapshot objects in a key:value store. In addition to storing these objects, provides an interface to quickly know which iMIs are store within the node that accommodates the repository. Every node in the platform (i.e. Master or Replica) must necessarily launch the one repository.
\end{description}


So that all the components presented above can function adequately, they require help from several libraries. A description of those implemented libraries ensue:

\begin{description}
	\item [BTRFS Lib] The BTRFS library holds two classes. The first, called \texttt{BTRFS Tool} that is a python wrapper for the \texttt{btrfs-progs} tool, described in section~\ref{par:incremental_btrfs}. The second, designated \texttt{BtrfsFsCheck} implements functions to validate if a given path is part of a BTRFS filesystem, case its true is also possible to verify if that path comprises a subvolume. Additionally, this tool is provided with a method to discover all snapshots in a directory.
	%
	\item [Compression Lib] Since multiple compression algorithms are used, makes all the sense to create a library that encapsulates multiple wrappers, one for each algorithm. These wrappers in reality only need to provide compression and decompression methods for a stream of data, with the remaining operations not present.
	%
	\item [Serialiser Lib] Some of the communications between nodes require the transmission of objects, to that effect a library containing serialisation and deserialisation methods for those objects is a requirement.
	%
	\item [SSH Lib] This library implements a wrapper for the SSH command, allowing the creation of a tunnel so that data can be funnelled through a secured connection between nodes in different networks.
	%
	\item [REST API Lib] To comply with one of the objectives of the replication module, a REST API should be provided. That is precisely what this library does, providing the endpoints to interact with the system. Also enabling an easy way to expand that interaction to other modules of the platform.

\end{description}

Even though the libraries presented above are not enough for the correct implementation of the all functionalities of the replication module, more libraries implemented by the community were necessary to handle aspects like communication between nodes, algorithms for data compression and to secure the data transmission between networks. Some of which we describe below and through the remaining text detailing each module.


%%-------------------------------------------------------------------
%%	4. - Communications between nodes
%%-------------------------------------------------------------------
\subsection{Communications between nodes}
\label{sub:rep_comms}

Coordinate the multiple modules and its activities, demands from the middleware a need to share a communication channel connected through a network. 
The remote procedure call (RPC) brings support for inter-process communication allowing a procedure on a system to invoke an operation running in a process in a different location, most likely on a remote system.

As seen in figure~\ref{fig:icbd_rep_architecture}, multiple processes are running in different machines any given time, and those processes need to continually send and receive information: perhaps operations to execute, metadata updates, or just monitoring if a process is running according to the desired plan or is in a faulty state.
Managing the nodes is a perfect case for the use of the Pyro 4 library, that gives a holistic view of the system and allows triggering a multitude of operations in each node.


%\subsubsection{Pyro4 Library}
\paragraph{Pyro4 Library}
\label{par:rep_pyro4}
%https://pythonhosted.org/Pyro4/
Pyro 4~\cite{pyro4} is a library that enables the development of python applications in which objects can talk to each other over the network through the use of RPCs. Its designed to be very easy to use and integrate into a project and at the same time provide considerable flexibility. This library can be imagined as an adhesive to integrate various components of a heterogeneous system easily.

There are some core features employed in the iCBD replication module, but not limited to:


\begin{description}
	\item [Proxy] This object acts as a substitute for the real one, intercepting the calls to a given method of that object. Then through the Pyro library, the call is sent to the actual object that probably resides in a remote machine, also returning the resulting data of the call. Which is very useful considering that the function that performs a call to the object does not need to know if it is dealing with a normal or a remote object.
	%
	\item [Pyro object] A Pyro object is a regular python object that goes through a registration process with Pyro in order to facilitate remote access to it. Objects are written just as any other piece of code, but the fact that Pyro knows its existence allows calls to that object that may originate in other programs.
	%
	\item [Pyro daemon] This component is the one that listens for remote method calls done to a proxy, dispatches them to the real object and collects the result of the call returning it to the caller.
	%
	\item [Name Server] Is this tool that keeps track of the objects actual locations in the network so they can move around freely and transparently. Works similarly to a yellow-pages book, providing lookups based on metadata tags.
	%
	\item [Automatic reconnecting] If a client (in our case a Replica Node) becomes disconnected to the server (Master Node), because of a server crash or a communications error, there is an automatic retry mechanism to handle this fault.
	%
	\item [Secure communication] Pyro itself does not encrypt by default the data it sends over the network. Still, Pyro RPCs communications can travel over a secure network (VPN, SSL/SSH tunnel) where the encryption is taken care of externally to the library. Alternatively, it is also possible to enable SSL/TLS in the Pyro library itself, securing all communications with custom cert files, private keys, and passwords.
	%
	\item [Serialisation] Pyro supports the transformation of objects into streams of bytes that flow over the network to a receiver that deserialises them back into the original format. This process is essential to ensure the delivery of all data that remote method calls receive as arguments, as well as the corresponding responses.
\end{description}


\paragraph{TCP Sockets and Secure Shell Protocol (SSH)}
\label{par:rep_tcp_sockets}
Despite the facts presented above, system coordination is not all the burden on the network side, the main task of this system is to replicate virtual machine images among the several nodes, so the network has responsibility on carrying large volumes of data. The Pyro4 library gives the possibility to secure its communications, but that only covers method calls within nodes. 

The delivery process of iMIs throughout nodes follow one of two principles: in the first scenario, we consider the case where the iMI does not leave the same trusted local network (i.e. communications within the walls of one organisation). The second implies the transport of data between third-party networks, even the internet. 

When talking about an organisational network, its safe to assume that there are some security measures already in place (e.g. VLANs), so in this regard, we pass the concerns about data security for this layer allowing the use of a simple Stream Socket which provides a connection-oriented flow of data with error detection, in our case implemented on top of TCP. The use of this type of socket, and the non-use of cyphers, allows the best performance in the transfer of an iMI without the addition of computationally heavy tasks such as encrypting a large amount of data.

In the second case, to solve the question of the data travelling through uncontrolled networks, an extension of the previous solution is presented, using the same type of socket but redirecting the flow through an SSH tunnel deployed between nodes. 

This solution in addition to solving the issue of ensuring data security in the transferal process is a modular solution that allows future changes in the way data is encrypted without needing significant modifications to the code base. Even so, we do not believe that this is a perfect security model, there is room for improvement, but, not being the focal point of this work, we still wanted to provide some security features for conducting functional tests linking geographically separated data centres.


\paragraph{Data Compression}
\label{par:rep_data_compression}

\begin{description}
	\item [LZ4] is a lossless data compression algorithm focused on compression and decompression speed. It belongs to the LZ77 family of byte-oriented compression schemes. The reference implementation in C by Yann Collet, there are also ports and bindings in various languages like Java, C\#, Python, among others.
	%
	\item [zlib] is widely used, a kind of a de facto standard on the data compression subject. The algorithm provides good compression on a wide variety of data with minimal use of system resources. Written in C, as the algorithms presented above is a variation of LZ77.
	%
	\item [Snappy] is a library written in C++ by Google, and as the LZ4 algorithm is built for fast data compression and decompression and based on ideas from LZ77.
\end{description}

%%-------------------------------------------------------------------
%%	4. - Name Server
%%-------------------------------------------------------------------
\subsection{Name Server}
\label{sub:rep_name_server}

\textbf{TOPICS :}
\begin{itemize}
	\item Starting Pyro4 Name Server from within iCBD-Replication code
	\item Configurations applied (ip / port)
	\item How the Name Server is controlled?
\end{itemize}


%The name server is backed by a sqlite persistent database.

%It is just a single (name,uri) table for the names and another table for the metadata.

\begin{listing}[h!]
\begin{minted}[frame=lines, framesep=2mm, fontsize=\footnotesize, linenos]{python}
class NameServer(Thread):

	def __init__(self, config):
		# Thread configs
        Thread.__init__(self, name="Thread-NameServer")
        self.ns_ip = config["ip"]
        self.ns_port = config["pyroPort"]

        # Pyro name server
        self.nameserver_uri, 
        self.nameserver_daemon, 
        self.broadcast_server = Pyro4.naming.startNS(host=self.ns_ip,
                                                     port=self.ns_port,
                                                     storage=DB_FILE)

        self.nameserver_daemon.combine(self.broadcast_server)


	def run(self):
        # This is triggered in the thread.start() call
        try:
            self.nameserver_daemon.requestLoop()
        finally:
            # clean up
            self.broadcast_server.close()
            self.nameserver_daemon.close()
\end{minted}
\caption{Starting procedure of a Name Server}
\label{listing:icbd_nameserver}
\end{listing}



%%-------------------------------------------------------------------
%%	4. - Image Repository
%%-------------------------------------------------------------------
\subsection{Image Repository}
\label{sub:rep_image_repo}

This module presents itself with a central role in the replication process, being responsible for knowing at any given time which iMIs are present in the platform and register the changes as they happen.
As explained before, the iCBD platform stores the multiple versions of one iMI as snapshots that materialise as directories in the filesystem. The Image Repository bridges the gap by maintaining a structure that is backed by an \textit{SQLite} persistent database. The interface is elementary, offering a hand full of mutator methods (get and set functions) to populate one data structure. This Dictionary as an unordered set of key: value pairs allows to establish as key the name of the iMI and the value a List with several \textit{icbdSnapshot} objects. 

%\subsubsection{iCBD Snapshot Object Structure (iMI)}
\paragraph{iCBD Snapshot Object Structure (iMI)}
\label{par:icbd_snapshot}

Inside the replication module, the iMI is treated as a first-class citizen, being represented by the class \textit{icbdSnapshot}. This object stores the relevant metadata and properties of an iMI that are essential to unequivocally distinguish the multiple images present in the system, not the actual raw data.

In that sense, from this object, we can obtain of curse the name of the iMI and its version number, the full path to the VM files in the filesystem, the location of the boot package regarding the version in question, and the configuration file for the iSCSI target. Since the data stored within this object is immutable since its creation, the object only provides \textit{get} functions to retrieve its values. 

\begin{listing}[h!]
\begin{minted}[frame=lines, framesep=2mm, fontsize=\footnotesize, linenos]{python}
class icbdSnapshot(object):
    def __init__(self, mount_point: str,
                 image_name: str,
                 snapshot_number: str,
                 creation_time: float,
                 icbd_boot_package_path: str,
                 iscsi_target_folder: str):

        self.mount_point = mount_point            # Path to the FS where the VM Files are stored
        self.image_name = image_name              # Name of the iMI
        self.snapshot_number = snapshot_number    # Version number
        self.creation_time = creation_time        # Moment the iMI was added to the platform

        # iMI Boot Package
        self.icbd_boot_package_path = icbd_boot_package_path

        # iMI iSCSI target
        self.iscsi_target_folder = iscsi_target_folder
        self.iscsi_target_name = "{}-{}_flat.conf".format(self.image_name, self.snapshot_number)\end{minted}
\caption{Example of the information stored in the \textit{icbdSnapshot} object.}
\label{listing:icbdSnapshot_example}
\end{listing}

%\begin{listing}[h!]
%\includegraphics[height=4in, width=7in]{carbon}
%\caption{Example of the i2222nformation stored in the \textit{icbdSnapshot} object.}
%\label{listing:icbdSnaps222hot_example}
%\end{listing}



%%-------------------------------------------------------------------
%%	4. - Master Node
%%-------------------------------------------------------------------
\subsection{Master Node}
\label{sub:rep_master_node}

\textbf{TOPICS :}
\begin{itemize}
	\item Startup routine, including the launch of other services
	\item Main controller for all nodes of iCBD-Replication
	\item The send operation 
\end{itemize}

\paragraph{Keep Alive}
\label{par:rep_keep_alive}

\textbf{TOPICS :}
\begin{itemize}
	\item Launched by the master node
	\item Periodically check if the replica nodes are still working
	\item Case any change talks to the Name Server to change things according
\end{itemize}

\subsubsection{CLI Interface}
\label{subsub:rep_cli_interface}


\subsubsection{REST API}
\label{subsub:rep_restapi}
\textbf{TOPICS :}
\begin{itemize}
	\item Flask lib
	\item Endpoints
\end{itemize}


%%-------------------------------------------------------------------
%%	4. - Replica Node
%%-------------------------------------------------------------------
\subsection{Replica Node}
\label{sub:rep_replica_node}

\textbf{TOPICS :}
\begin{itemize}
	\item Operations on the local repository
	\item The receive operation
\end{itemize}




%%-------------------------------------------------------------------
%%	4. - Building a iCBD Cache Server
%%-------------------------------------------------------------------
\section{Building a iCBD Cache Server}
\label{sec:impl_cache_server}

%To solve some of the enunciated problems with a DaaS solution that derive from limited bandwidth, latency and jitter from the limitation of accessing the image repositories from an internet connection and provide some scalability feature with the implementation of proximity cache servers are key. These cache servers can store replicas of the iMIs created and maintained in an administration server. Moreover, since they are located in the same LAN segment as the clients is from here that they will boot.
%For accomplishing this work, the cache servers need to have hard drives. (Although it would be possible to have diskless cache servers, they would be blocked if there was an interruption in the internet access, and it is to avoid that the local drives are necessary. )

%A instalação, configuração e administração dos cache-servers far-se-á também pela instanciação a partir da imagem de uma VM especialmente preparada para o efeito e residente na cloud, no sistema de administração. Os cache-servers arrancam inicialmente pela rede a partir do servidor na cloud e carregam um Linux que vai formatar os discos, neles instalando o conteúdo da própria imagem de VM, terminando com a instalação de um boot loader que irá arrancar o sistema do cache-server após a máquina fazer reboot. Daí em diante o sistema de administração na cloud irá disponibilizar as actualizações que forem necessárias, podendo mesmo forçar a re-instalação total dos cache-servers.

%Note-se que uma vez instalado um cache-server numa LAN, dada a grande fiabilidade que pode ser ainda reforçada através das técnicas habituais em sistemas tolerantes a faltas e de elevada disponibilidade (referir a sugerida pelo Paulo), torna-se viável a utilização de servidores diskless, instanciados a partir de VMs templates configuradas e administradas na cloud e depois replicadas para o cache-server, como nas imagens dos desktops.

%Assim como o Linux dos cache-servers é instalado nos discos locais dos servidores a partir de imagens na cloud, é possível fazer o mesmo com qualquer outra imagem de Linux que se queira. Assim, o administrador de sistema de um cliente poderá configurar outras VMs com o software e os serviços de que necessitar, designar uma máquina física como alvo, e fazer com que a VM seja vertida para os discos da sua máquina física, sendo configurado um boot loader para lhe permitir arrancar com essa configuração.

%Como os clientes nestas categorias tipicamente necessitam de dezenas, centenas ou mais de postos de trabalho, na prática é impossível suportá-los directamente a partir de repositórios remotos de VMs templates. Em vez disso, iremos recorrer a “servidores de proximidade” ou “c​ache-servers”​, máquinas Linux reais ou virtuais devidamente dimensionadas e configuradas, instaladas na LAN dos clientes, idealmente uma (ou mais) por cada segmento de rede, as quais manterão réplicas das VMs templates a que cada cliente tem acesso nos repositórios remotos.
%Com estes cache-servers localmente acessíveis aumentar-se-á drasticamente a velocidade de transferência de dados e a latência e o jitter nos acessos reduzir-se-ão a valores insignificantes, podendo cada cache-server suportar dezenas (ou mais) de PCs clientes (estimamos o número em 15 a 30 por cada interface gigabit nos cache-servers, estimativa que permanece válido para PCs clientes ligados a 100 Mbps através de switches com ligação gigabit aos cache-servers).


\subsection{The infrastructure}
\label{sub:infrastructure}

\begin{table}[htpb]
\centering
\begin{tabular}{lcc}
\hline
                               & \textbf{FCT NOVA}          & \textbf{Reditus}           \\ \hline
\textit{\textbf{Servers}}      & 2 x HP ProLiant DL380 Gen9 & 2 x HP ProLiant DL380 Gen9 \\
\textit{\textbf{Switch}}       & HPE Flexfabric 5700 jg898a & HPE Flexfabric 5700 jg898a \\
\textit{\textbf{Disk Array}}   & HPE MSA 2040 SAN Storage   & -                          \\
\textit{\textbf{WorkStations}} & 2 Labs (15 PCs each)       & 3 Test PCs                 \\
\textit{\textbf{Networking}}   & 1 Gbps                     & 1 Gbps                     \\ \hline
\end{tabular}
\caption{Physical infrastructure of the FCT NOVA and SolidNetworks sites}
\end{table}

\paragraph{Network}
\label{par:infra_network}


\subsection{System Overview}
\label{sub:system_overview}

%%-------------------------------------------------------------------
%%	4. - Services
%%-------------------------------------------------------------------
\subsection{Services}
\label{sub:cache_services}

\paragraph{VM Roles in the Platform}
\label{par:vm_roles}

\begin{description}
	\item [\textit{iCBD-imgs}]
	\item [\textit{iCBD-rw}]
	\item [\textit{iCBD-home}]
	\item [\textit{iCBD-cache}]
\end{description}


\paragraph{Installing iCBD Core Services}
\label{par:install_icbd_core}
%Found a Centos 7 kernel bug.
%https://bugs.centos.org/view.php?id=14228
%https://bugzilla.redhat.com/show_bug.cgi

\paragraph{BTRFS bug found in CentOS 7 Kernel}
\label{par:centos_bug}

As a curiosity during the process of building the \textit{iCBDimgs} VM, we found in a bug in a core component of the \textit{coreutils} tool introduced in the kernel version \texttt{3.10.0-693.5.2} of CentOS 7. The \texttt{cp} command when used with option \texttt{--reflink=always} fails with the indication "failed to clone 'someFile': Operation not supported". This behaviour was reported to both CentOS and Red Hat and was eventually resolved. A more in-depth description of this process can be found in Annex~\ref{ann:bug}.







