%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter6.tex
%% NOVA thesis document file
%%
%% Chapter with lots of dummy text
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions \& Future Work}
\label{cha:conclusion}

\section{Conclusions}
\label{sec:con_conclusions}

%\textbf{TOPICS :}
%\begin{itemize}
%	\item The requirements of the implementation of the iCBD-Replication were achieved 
%	\item Simple replication of iMI though multiple nodes with a subscription model, where the node only receives the iMIs that really want.
%	\item Successful creation of a complete iCBD platform in FCT NOVA campus.
%	\item Verifica-se que a introdução do cache server aumenta um pouco o tempo de boot dos clientes, mas mais importante verifica-se capaz de aguentar o boot simultâneo de um Lab. completo ficando pelos 20\% de ocupação de CPU. Verificando-se assim uma alta escalabilidade horizontal da solução.
%	\item Two department lab running the solution as a trial, and instrumental in getting experimental results
%\end{itemize}


%The work developed by this thesis introduced two new components to the iCBD platform, a replication mechanism following a subscription model that allows the propagation of iMI by several replicas, and the concept of Cache Server that paves a way to support a large number of clients.

%We show that the implementation of the replication system of iMIs in a Master - Replica model not only provides a process for sending iMIs in a geographically dispersed and multi-server environment, delivering network fault tolerance properties, but also a control layer that simplifies the entire process related to the government of the state of each node. And still being modular to the point of allowing the possibility of being easily extendable to other file systems other than Btrfs.

%We discussed the complete installation process of the iCBD platform, from root, in the infrastructure of the Department of Computer Science of FCT NOVA, detailing some of the challenges encountered. All this work culminated in the elaboration of an installation manual and demonstrating the feasibility of integrating a Cache Server into the iCBD platform.

%Finally, we evaluated both components of this work by performing a comprehensive functional validation, and by designing a benchmark to the system performance of both replication and caching systems. Being that the results demonstrated total feasibility of the solutions proposed, having been achieved the integration with the work already developed in the iCBD platform. 

%In the replication system, we saw that the solution presented meets the proposed requirements and obtains performance levels in line with what was expected. Regarding the introduction of a Cache Server on the platform, the results consistently show a more considerable boot time when comparing to clients that boot from a machine with more significant resources. Which is mainly due to the cache server possessing hardware not much better than a workstation, we can discern that when compared to a traditional server-based VDI solution~\cite{Lopes2017_presentation}, the load on this component, even in a boot storm scenario, does not exceed 20\%. This fact leads us to conclude that the platform can be scaled horizontally.

In this dissertation, we have architected, designed, implemented and benchmarked a distribute Replica and Cache System (RCS) to be integrated in the iCBD platform, which, in our perspective, satisfies all the goals we set out to achieve.

Our implementation was conducted with Btrfs in mind, but our modular design follows the well-known publish-subscribe model that, we believe, allows for an easy extension to other suitable storage platforms, even those which are based on a distinct paradigm, e.g., object-based storage.

We have shown that our implementation of a replication system for iMIs based on a Master - Replica model provides an efficient process for sending iMIs to their target locations in a geographically dispersed, multi-server environment, and guarantees some degree of fault-tolerance to the users of the iCBD platform. As for the RCS’ control layer, it simplifies the entire process maintaining the state of the nodes.

We have documented the complete installation process of the iCBD platform, right from the start, in the DI-FCT NOVA site, and we overcome the challenges we faced; most of these challenges can also be found in a typical SME, so we believe that the information in the manual is applicable to other environments, outside academia. All this work culminated in an “Installation Manual” and has demonstrated the feasibility of integrating a Cache Server into the iCBD platform.

Finally, we evaluated both the Replication and the Cache components through a comprehensive functional validation, and we designed a benchmark to assess the performance of both the replication and the caching systems. In our opinion, the results clearly demonstrate the benefits of the proposed solutions.

In the replication system, we saw that the solution presented satisfies the requirements and has a level of performance that is in line with what was expected.

In the case of the physical Cache Server, results consistently show a considerable increase in the boot time when compared to booting from a virtual one (VM) which was granted with far more resources. In our opinion, this is an excellent result, as it shows that if Cache Server nodes are vertically scalable (i.e., more resources translate to higher performance levels) and, as the service is already horizontally scalable by design, we have a solution that is scalable in all dimensions. Obviously, the poor result of the physical Cache Server used in the benchmark is a consequence of: a) utilising a PC and not a server and, b) on top of that the amount of RAM and number of CPUs was below par.

In the end, all our work validates the main ideas behind iCBD: it is, by design, an horizontally scalable architecture that, from the point of view of capital costs, may be well at the reach of SMBs. 

\section{Future Work}
\label{sec:con_future_work}

After spending many months working with iCBD, and not only for the immediate end of getting our masters degree, several ideas for future development have been popping up along the way.

The first one is to replace the Master-Server model used in the replication system by a peer-to-peer model, where replica nodes can “talk to each other”, allowing the transfer of iMI versions directly between replicas. This scenario would be especially advantageous in a circumstance where a Master Node was located in a geographic area far away from the Replicas, or even hosted in a public cloud. With several Replica Nodes installed on the same network, only one of the Replica nodes needed to receive the new version of the iMI; then that same node would send it directly to the other Replicas, possibly in a multicast or broadcast mode.

Another idea that was partially adopted in some services in the current implementation is the segmentation of iCBD’s multiple components into microservices, where each microservice is able to operate autonomously and coordinate itself with others using message exchange or any other interprocess communication mechanism. With this change, we believe that it would be possible to make better use of the platform resources and, even more relevant, microservices – implemented, e.g., as containers - do benefit from platform support for launching services independently of their location, as well as simplifying the development and deployment of new or updated platform services in parallel, without affecting the platform as a whole – e.g., these new versions can be deployed “live”. These approaches are usually referred to as DevOps.


%During the process of development of this work, several ideas for future developments in this project came up.

%Taking advantage of the extensibility of the assembled system for iMI replication, it could be extended in such a way that the replicas may communicate with each other, allowing the transfer of versions of iMIs directly between replicas. This scenario would be especially advantageous in a circumstance where the Master Node is located in a geographic location far way from the Replicas or even hosted in a public cloud. With several Replica Nodes installed on the same network, only one of the nodes needed to receive a new version of the iMI then he would send it directly to the other Replicas.

%Another idea that started to be implemented in the modularity of the installation of the iCBD platform on the FCT NOVA site is the segmentation of the multiple components into microservices, where each one is able to operate autonomously using a kind of inter-process communication mechanism or merely the network itself. With this change, it is believed that it would be possible to make better use of the resources allocated to the platform, and probably even more important is the fact that it provides the support for launching services independently of their location, as well as facilitating the development in parallel of different features of the platform without affecting the platform as a whole.

%\textbf{TOPICS :}
%\begin{itemize}
%	\item Replica to Replica iMI transfer
%	\item Diskless Servers / selfhosting - provision iCBD Servers from iMI
%	\item Micro Services, as started with this thesis (building iCBD-Replication as a standalone application) the functionalities of the iCBD Core can be segmented in multiple small services, in order to achieve a better use of resources and an easier deployment in a multi homed scenario.
%	\item Infrastructure as Code, orchestrate and automate all the process described in the chapter Implementation of a Cache Server.
%\end{itemize}




